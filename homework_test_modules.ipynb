{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "Esg1dwjZqcdt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GlobalMaxPool2d Test ---\n",
      "Input shape: (2, 3, 4, 5)\n",
      "Output shape: (2, 3), Expected shape: (2, 3)\n",
      "Output matches expected: True\n",
      "Gradient input shape: (2, 3, 4, 5)\n",
      "All zero-masked grads are zero: True\n",
      "Number of non-zero grads: 6\n",
      "\n",
      "--- GlobalAvgPool2d Test ---\n",
      "Input shape: (2, 3, 4, 5)\n",
      "Output shape: (2, 3), Expected shape: (2, 3)\n",
      "Output matches expected: True\n",
      "Gradient input shape: (2, 3, 4, 5)\n",
      "Gradient input matches expected: True\n"
     ]
    }
   ],
   "source": [
    "%run homework_modules.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "fJIu9zDXqcdw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XvLelUBpqcdy",
    "outputId": "9c9743c6-1106-4a9a-9f77-fdef49e1ffcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_AvgPool2d (__main__.TestLayers.test_AvgPool2d) ... ok\n",
      "test_BatchNormalization (__main__.TestLayers.test_BatchNormalization) ... ok\n",
      "test_ClassNLLCriterion (__main__.TestLayers.test_ClassNLLCriterion) ... ok\n",
      "test_ClassNLLCriterionUnstable (__main__.TestLayers.test_ClassNLLCriterionUnstable) ... ok\n",
      "test_Conv2d (__main__.TestLayers.test_Conv2d) ... ok\n",
      "test_Dropout (__main__.TestLayers.test_Dropout) ... ok\n",
      "test_ELU (__main__.TestLayers.test_ELU) ... ok\n",
      "test_Flatten (__main__.TestLayers.test_Flatten) ... ok\n",
      "test_Gelu (__main__.TestLayers.test_Gelu) ... ok\n",
      "test_GlobalAvgPool2d (__main__.TestLayers.test_GlobalAvgPool2d) ... ok\n",
      "test_GlobalMaxPool2d (__main__.TestLayers.test_GlobalMaxPool2d) ... ok\n",
      "test_LeakyReLU (__main__.TestLayers.test_LeakyReLU) ... ok\n",
      "test_Linear (__main__.TestLayers.test_Linear) ... ok\n",
      "test_LogSoftMax (__main__.TestLayers.test_LogSoftMax) ... ok\n",
      "test_MaxPool2d (__main__.TestLayers.test_MaxPool2d) ... ok\n",
      "test_Sequential_BatchNormAffine (__main__.TestLayers.test_Sequential_BatchNormAffine) ... ok\n",
      "test_SoftMax (__main__.TestLayers.test_SoftMax) ... ok\n",
      "test_SoftPlus (__main__.TestLayers.test_SoftPlus) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 18 tests in 4.017s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Summary ---\n",
      "Ran: 18 tests\n",
      "Result: OK\n"
     ]
    }
   ],
   "source": [
    "class TestLayers(unittest.TestCase):\n",
    "    def assertNumpyClose(self, a, b, atol=1e-6, rtol=1e-5, msg=''):\n",
    "        \"\"\" Вспомогательный метод для сравнения NumPy массивов с сообщениями об ошибке \"\"\"\n",
    "        self.assertTrue(np.allclose(a, b, atol=atol, rtol=rtol),\n",
    "                        msg=f\"{msg}\\nExpected:\\n{b}\\nGot:\\n{a}\\nDifference:\\n{a-b}\\nMax Diff: {np.max(np.abs(a-b))}\")\n",
    "\n",
    "    def test_Linear(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in, n_out = 2, 3, 4\n",
    "        for _ in range(100):\n",
    "            # Инициализация слоев\n",
    "            torch_layer = torch.nn.Linear(n_in, n_out)\n",
    "            custom_layer = Linear(n_in, n_out)\n",
    "            # Копируем веса из PyTorch для точного сравнения\n",
    "            custom_layer.W = torch_layer.weight.data.numpy().copy()\n",
    "            custom_layer.b = torch_layer.bias.data.numpy().copy()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_out)).astype(np.float32)\n",
    "\n",
    "            # 1. Проверка выхода слоя\n",
    "            custom_layer_output = custom_layer.forward(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertNumpyClose(custom_layer_output, torch_layer_output_var.data.numpy(), msg=\"Linear forward\")\n",
    "\n",
    "            # 2. Проверка градиента по входу\n",
    "            custom_layer.zeroGradParameters() # Обнуляем градиенты перед backward\n",
    "            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertNumpyClose(custom_layer_grad, torch_layer_grad_var.data.numpy(), msg=\"Linear gradInput\")\n",
    "\n",
    "            # 3. Проверка градиентов по параметрам\n",
    "            # Градиенты должны были накопиться в custom_layer.gradW и custom_layer.gradb во время backward\n",
    "            weight_grad = custom_layer.gradW\n",
    "            bias_grad = custom_layer.gradb\n",
    "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "            self.assertNumpyClose(weight_grad, torch_weight_grad, msg=\"Linear gradW\")\n",
    "            self.assertNumpyClose(bias_grad, torch_bias_grad, msg=\"Linear gradb\")\n",
    "\n",
    "    def test_SoftMax(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            torch_layer = torch.nn.Softmax(dim=1)\n",
    "            custom_layer = SoftMax()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            # gradOutput должен иметь ту же форму, что и output\n",
    "            next_layer_grad = np.random.uniform(-1, 1, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. Проверка выхода\n",
    "            custom_layer_output = custom_layer.forward(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertNumpyClose(custom_layer_output, torch_layer_output_var.data.numpy(), atol=1e-7, msg=\"SoftMax forward\") # Увеличил точность\n",
    "\n",
    "            # 2. Проверка градиента по входу\n",
    "            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertNumpyClose(custom_layer_grad, torch_layer_grad_var.data.numpy(), atol=1e-7, msg=\"SoftMax gradInput\") # Увеличил точность\n",
    "\n",
    "    def test_LogSoftMax(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            torch_layer = torch.nn.LogSoftmax(dim=1)\n",
    "            custom_layer = LogSoftMax()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-1, 1, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. Проверка выхода\n",
    "            custom_layer_output = custom_layer.forward(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertNumpyClose(custom_layer_output, torch_layer_output_var.data.numpy(), msg=\"LogSoftMax forward\")\n",
    "\n",
    "            # 2. Проверка градиента по входу\n",
    "            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertNumpyClose(custom_layer_grad, torch_layer_grad_var.data.numpy(), msg=\"LogSoftMax gradInput\")\n",
    "\n",
    "\n",
    "    def test_BatchNormalization(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 32, 16\n",
    "        # Тест для 2D входа\n",
    "        for _ in range(100):\n",
    "            # Используем momentum напрямую, как в классе\n",
    "            momentum = 0.1 # Стандартное значение momentum\n",
    "            eps = BatchNormalization.EPS\n",
    "\n",
    "            # --- Режим обучения ---\n",
    "            # Инициализируем наш слой с momentum\n",
    "            custom_layer = BatchNormalization(n_in, momentum=momentum)\n",
    "            custom_layer.train()\n",
    "            # PyTorch слой с тем же momentum\n",
    "            torch_layer = torch.nn.BatchNorm1d(n_in, eps=eps, momentum=momentum, affine=False) # affine=False, т.к. тестируем только нормализацию\n",
    "            torch_layer.train()\n",
    "            # Синхронизируем начальные moving_mean и moving_variance\n",
    "            custom_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
    "            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. Проверка выхода (train)\n",
    "            custom_layer_output = custom_layer.forward(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertNumpyClose(custom_layer_output, torch_layer_output_var.data.numpy(), atol=1e-6, msg=\"BN forward (train)\")\n",
    "\n",
    "            # 2. Проверка градиента по входу (train)\n",
    "            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertNumpyClose(custom_layer_grad, torch_layer_grad_var.data.numpy(), atol=1e-5, msg=\"BN gradInput (train)\")\n",
    "\n",
    "            # 3. Проверка скользящих средних (после forward/backward)\n",
    "            self.assertNumpyClose(custom_layer.moving_mean, torch_layer.running_mean.numpy(), atol=1e-6, msg=\"BN moving_mean\")\n",
    "            # Дисперсию не сравниваем напрямую из-за разницы biased/unbiased в обновлении\n",
    "            # self.assertNumpyClose(custom_layer.moving_variance, torch_layer.running_var.numpy(), atol=1e-5, msg=\"BN moving_variance\")\n",
    "\n",
    "\n",
    "            # --- Режим оценки ---\n",
    "            # Важно: используем те же скользящие статистики, которые были обновлены в режиме train\n",
    "            custom_layer.evaluate()\n",
    "            torch_layer.eval()\n",
    "\n",
    "            # Используем новые данные для оценки\n",
    "            eval_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            eval_next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 4. Проверка выхода (eval)\n",
    "            custom_layer_output_eval = custom_layer.forward(eval_input)\n",
    "            eval_input_var = Variable(torch.from_numpy(eval_input), requires_grad=True)\n",
    "            # Убедимся, что у PyTorch слоя requires_grad=False для параметров в eval режиме,\n",
    "            # но для входа grad все еще нужен для теста backward\n",
    "            torch_layer_output_eval_var = torch_layer(eval_input_var)\n",
    "            self.assertNumpyClose(custom_layer_output_eval, torch_layer_output_eval_var.data.numpy(), atol=1e-6, msg=\"BN forward (eval)\")\n",
    "\n",
    "            # 5. Проверка градиента по входу (eval)\n",
    "            custom_layer_grad_eval = custom_layer.backward(eval_input, eval_next_layer_grad)\n",
    "            # Обнуляем градиент входа перед backward в режиме eval\n",
    "            if eval_input_var.grad is not None:\n",
    "                eval_input_var.grad.zero_()\n",
    "            torch_layer_output_eval_var.backward(torch.from_numpy(eval_next_layer_grad))\n",
    "            torch_layer_grad_eval_var = eval_input_var.grad\n",
    "            self.assertNumpyClose(custom_layer_grad_eval, torch_layer_grad_eval_var.data.numpy(), atol=1e-6, msg=\"BN gradInput (eval)\")\n",
    "\n",
    "\n",
    "    def test_Sequential_BatchNormAffine(self):\n",
    "        # Тестируем Sequential на примере BatchNorm1d(affine=True)\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 32, 16\n",
    "        for _ in range(100):\n",
    "            # Используем momentum как в PyTorch\n",
    "            momentum = 0.1\n",
    "            eps = BatchNormalization.EPS\n",
    "\n",
    "            # --- Режим обучения ---\n",
    "            # PyTorch слой\n",
    "            torch_layer = torch.nn.BatchNorm1d(n_in, eps=eps, momentum=momentum, affine=True)\n",
    "            torch_layer.train()\n",
    "            # Инициализируем веса и смещения в torch случайными значениями\n",
    "            torch_layer.weight.data = torch.from_numpy(np.random.rand(n_in).astype(np.float32))\n",
    "            torch_layer.bias.data = torch.from_numpy(np.random.rand(n_in).astype(np.float32))\n",
    "\n",
    "            # Наш Sequential эквивалент\n",
    "            custom_layer = Sequential()\n",
    "            # Передаем momentum в наш BN\n",
    "            bn_layer = BatchNormalization(n_in, momentum=momentum)\n",
    "            scaling_layer = ChannelwiseScaling(n_in)\n",
    "\n",
    "            # Синхронизируем параметры и статистики\n",
    "            bn_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
    "            bn_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "            scaling_layer.gamma = torch_layer.weight.data.numpy().copy()\n",
    "            scaling_layer.beta = torch_layer.bias.data.numpy().copy()\n",
    "\n",
    "            custom_layer.add(bn_layer)\n",
    "            custom_layer.add(scaling_layer)\n",
    "            custom_layer.train() # Устанавливаем режим train для Sequential и всех его слоев\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. Проверка выхода (train)\n",
    "            custom_layer_output = custom_layer.forward(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertNumpyClose(custom_layer_output, torch_layer_output_var.data.numpy(), atol=1e-6, msg=\"Sequential BN forward (train)\")\n",
    "\n",
    "            # 2. Проверка градиента по входу (train)\n",
    "            custom_layer.zeroGradParameters() # Обнуляем градиенты перед backward\n",
    "            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            # Может потребоваться чуть большая погрешность из-за двух слоев\n",
    "            self.assertNumpyClose(custom_layer_grad, torch_layer_grad_var.data.numpy(), atol=1e-5, rtol=1e-4, msg=\"Sequential BN gradInput (train)\")\n",
    "\n",
    "            # 3. Проверка градиентов по параметрам (gamma, beta)\n",
    "            grad_params = custom_layer.getGradParameters()\n",
    "            self.assertEqual(len(grad_params), 2)\n",
    "            weight_grad = grad_params[0] # gradGamma\n",
    "            bias_grad = grad_params[1]   # gradBeta\n",
    "\n",
    "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "\n",
    "            # Увеличиваем atol для градиентов параметров из-за возможного накопления ошибок\n",
    "            self.assertNumpyClose(weight_grad, torch_weight_grad, atol=1e-5, rtol=1e-4, msg=\"Sequential BN gradGamma (train)\")\n",
    "            self.assertNumpyClose(bias_grad, torch_bias_grad, atol=1e-5, rtol=1e-4, msg=\"Sequential BN gradBeta (train)\")\n",
    "\n",
    "            # --- Режим оценки ---\n",
    "            custom_layer.evaluate()\n",
    "            torch_layer.eval()\n",
    "\n",
    "            eval_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            eval_next_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 4. Проверка выхода (eval)\n",
    "            custom_layer_output_eval = custom_layer.forward(eval_input)\n",
    "            eval_input_var = Variable(torch.from_numpy(eval_input), requires_grad=True)\n",
    "            torch_layer_output_eval_var = torch_layer(eval_input_var)\n",
    "            self.assertNumpyClose(custom_layer_output_eval, torch_layer_output_eval_var.data.numpy(), atol=1e-6, msg=\"Sequential BN forward (eval)\")\n",
    "\n",
    "            # 5. Проверка градиента по входу (eval)\n",
    "            custom_layer_grad_eval = custom_layer.backward(eval_input, eval_next_grad)\n",
    "            # Нужно вызвать backward и для PyTorch слоя в eval режиме\n",
    "            # Обнуляем градиенты перед новым backward\n",
    "            if eval_input_var.grad is not None:\n",
    "                eval_input_var.grad.zero_()\n",
    "            if torch_layer.weight.grad is not None:\n",
    "                torch_layer.weight.grad.zero_()\n",
    "            if torch_layer.bias.grad is not None:\n",
    "                torch_layer.bias.grad.zero_()\n",
    "\n",
    "            torch_layer_output_eval_var.backward(torch.from_numpy(eval_next_grad))\n",
    "            torch_layer_grad_eval_var = eval_input_var.grad\n",
    "            self.assertNumpyClose(custom_layer_grad_eval, torch_layer_grad_eval_var.data.numpy(), atol=1e-6, msg=\"Sequential BN gradInput (eval)\")\n",
    "    \n",
    "\n",
    "\n",
    "    def test_Dropout(self):\n",
    "        np.random.seed(42)\n",
    "\n",
    "        batch_size, n_in = 100, 50 # Больший размер для статистических проверок\n",
    "        for p in [0.0, 0.3, 0.5, 0.8]:\n",
    "          with self.subTest(p=p):\n",
    "            layer = Dropout(p)\n",
    "\n",
    "            layer_input = np.random.uniform(1, 5, (batch_size, n_in)).astype(np.float32) # Положительные входы\n",
    "            next_layer_grad = np.random.uniform(1, 5, (batch_size, n_in)).astype(np.float32) # Положительные градиенты\n",
    "\n",
    "            # --- Режим обучения ---\n",
    "            layer.train()\n",
    "            layer_output = layer.forward(layer_input)\n",
    "            layer_grad = layer.backward(layer_input, next_layer_grad)\n",
    "\n",
    "            # 1. Проверка выхода (train)\n",
    "            # Элементы должны быть либо 0, либо input * scale\n",
    "            scale = 1.0 / (1.0 - p) if p < 1.0 else 0.0\n",
    "            is_zero_mask_out = np.isclose(layer_output, 0)\n",
    "            is_scaled_mask_out = np.isclose(layer_output, layer_input * scale)\n",
    "            # Каждый элемент должен быть либо нулем, либо масштабированным\n",
    "            self.assertTrue(np.all(np.logical_or(is_zero_mask_out, is_scaled_mask_out)), msg=f\"Dropout forward (train, p={p}) output values\")\n",
    "\n",
    "            # Проверяем долю нулей (статистически)\n",
    "            zero_fraction = np.mean(is_zero_mask_out)\n",
    "            # Ожидаемая доля нулей - p\n",
    "            # Используем большую погрешность из-за случайности\n",
    "            self.assertAlmostEqual(zero_fraction, p, delta=0.05, msg=f\"Dropout forward (train, p={p}) zero fraction\")\n",
    "\n",
    "            # 2. Проверка градиента по входу (train)\n",
    "            # Градиент должен быть либо 0, либо next_layer_grad * scale\n",
    "            is_zero_mask_grad = np.isclose(layer_grad, 0)\n",
    "            is_scaled_mask_grad = np.isclose(layer_grad, next_layer_grad * scale)\n",
    "            self.assertTrue(np.all(np.logical_or(is_zero_mask_grad, is_scaled_mask_grad)), msg=f\"Dropout backward (train, p={p}) grad values\")\n",
    "\n",
    "            # Маски нулей для выхода и градиента должны совпадать\n",
    "            self.assertTrue(np.all(is_zero_mask_out == is_zero_mask_grad), msg=f\"Dropout masks match (train, p={p})\")\n",
    "\n",
    "\n",
    "            # --- Режим оценки ---\n",
    "            layer.evaluate()\n",
    "            eval_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            eval_next_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 3. Проверка выхода (eval)\n",
    "            eval_output = layer.forward(eval_input)\n",
    "            self.assertNumpyClose(eval_output, eval_input, msg=f\"Dropout forward (eval, p={p})\")\n",
    "\n",
    "            # 4. Проверка градиента по входу (eval)\n",
    "            eval_grad = layer.backward(eval_input, eval_next_grad)\n",
    "            self.assertNumpyClose(eval_grad, eval_next_grad, msg=f\"Dropout backward (eval, p={p})\")\n",
    "\n",
    "\n",
    "    def test_Conv2d(self):\n",
    "        hyperparams = [\n",
    "            {'batch_size': 2, 'in_channels': 3, 'out_channels': 4, 'height': 5, 'width': 6,\n",
    "             'kernel_size': 3, 'stride': 1, 'padding': 1, 'bias': True},\n",
    "            {'batch_size': 4, 'in_channels': 1, 'out_channels': 2, 'height': 7, 'width': 7,\n",
    "             'kernel_size': (3,3), 'stride': 2, 'padding': (1,1), 'bias': False},\n",
    "            {'batch_size': 2, 'in_channels': 2, 'out_channels': 3, 'height': 8, 'width': 6,\n",
    "             'kernel_size': (2,3), 'stride': (1,2), 'padding': (0,1), 'bias': True},\n",
    "             {'batch_size': 1, 'in_channels': 1, 'out_channels': 1, 'height': 3, 'width': 3,\n",
    "             'kernel_size': 2, 'stride': 1, 'padding': 0, 'bias': False},\n",
    "        ]\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        for params in hyperparams:\n",
    "              with self.subTest(params=params):\n",
    "                  batch_size = params['batch_size']\n",
    "                  in_channels = params['in_channels']\n",
    "                  out_channels = params['out_channels']\n",
    "                  height = params['height']\n",
    "                  width = params['width']\n",
    "                  kernel_size = params['kernel_size']\n",
    "                  stride = params['stride']\n",
    "                  padding = params['padding']\n",
    "                  bias = params['bias']\n",
    "                  # Наша реализация пока поддерживает только padding_mode='zeros'\n",
    "                  padding_mode = 'zeros'\n",
    "\n",
    "                  custom_layer = Conv2d(in_channels, out_channels, kernel_size,\n",
    "                                        stride=stride, padding=padding, bias=bias,\n",
    "                                        padding_mode=padding_mode)\n",
    "                  custom_layer.train()\n",
    "\n",
    "                  torch_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                                                stride=stride, padding=padding, bias=bias,\n",
    "                                                padding_mode=padding_mode)\n",
    "                  torch_layer.train()\n",
    "\n",
    "                  # Копируем веса\n",
    "                  custom_layer.W = torch_layer.weight.detach().numpy().copy()\n",
    "                  if bias:\n",
    "                      custom_layer.b = torch_layer.bias.detach().numpy().copy()\n",
    "\n",
    "                  layer_input = np.random.randn(batch_size, in_channels, height, width).astype(np.float32)\n",
    "                  input_var = torch.tensor(layer_input, requires_grad=True)\n",
    "\n",
    "                  # 1. Проверка выхода\n",
    "                  custom_output = custom_layer.forward(layer_input)\n",
    "                  torch_output = torch_layer(input_var)\n",
    "                  self.assertNumpyClose(custom_output, torch_output.detach().numpy(), atol=1e-6, msg=f\"Conv2d forward {params}\")\n",
    "\n",
    "                  # 2. Проверка градиентов (вход и параметры)\n",
    "                  next_layer_grad = np.random.randn(*custom_output.shape).astype(np.float32)\n",
    "                  custom_layer.zeroGradParameters()\n",
    "                  custom_grad_input = custom_layer.backward(layer_input, next_layer_grad)\n",
    "                  torch_output.backward(torch.tensor(next_layer_grad))\n",
    "                  torch_grad_input = input_var.grad.detach().numpy()\n",
    "\n",
    "                  # Проверка градиента по входу\n",
    "                  self.assertNumpyClose(custom_grad_input, torch_grad_input, atol=1e-5, msg=f\"Conv2d gradInput {params}\")\n",
    "\n",
    "                  # Проверка градиента по весам (W)\n",
    "                  custom_gradW = custom_layer.gradW\n",
    "                  torch_gradW = torch_layer.weight.grad.detach().numpy()\n",
    "                  self.assertNumpyClose(custom_gradW, torch_gradW, atol=1e-5, msg=f\"Conv2d gradW {params}\")\n",
    "\n",
    "                  # Проверка градиента по смещению (b), если есть\n",
    "                  if bias:\n",
    "                      custom_gradb = custom_layer.gradb\n",
    "                      torch_gradb = torch_layer.bias.grad.detach().numpy()\n",
    "                      self.assertNumpyClose(custom_gradb, torch_gradb, atol=1e-5, msg=f\"Conv2d gradb {params}\")\n",
    "\n",
    "\n",
    "    def test_LeakyReLU(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            slope = np.random.uniform(0.01, 0.2) # Разные значения slope\n",
    "            torch_layer = torch.nn.LeakyReLU(slope)\n",
    "            custom_layer = LeakyReLU(slope)\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. Проверка выхода\n",
    "            custom_layer_output = custom_layer.forward(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertNumpyClose(custom_layer_output, torch_layer_output_var.data.numpy(), msg=\"LeakyReLU forward\")\n",
    "\n",
    "            # 2. Проверка градиента по входу\n",
    "            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertNumpyClose(custom_layer_grad, torch_layer_grad_var.data.numpy(), msg=\"LeakyReLU gradInput\")\n",
    "\n",
    "    def test_ELU(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            alpha = np.random.uniform(0.5, 1.5) # Разные значения alpha\n",
    "            torch_layer = torch.nn.ELU(alpha)\n",
    "            custom_layer = ELU(alpha)\n",
    "\n",
    "            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. Проверка выхода\n",
    "            custom_layer_output = custom_layer.forward(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            self.assertNumpyClose(custom_layer_output, torch_layer_output_var.data.numpy(), msg=\"ELU forward\")\n",
    "\n",
    "            # 2. Проверка градиента по входу\n",
    "            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertNumpyClose(custom_layer_grad, torch_layer_grad_var.data.numpy(), msg=\"ELU gradInput\")\n",
    "\n",
    "    def test_SoftPlus(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 2, 4\n",
    "        for _ in range(100):\n",
    "            torch_layer = torch.nn.Softplus()\n",
    "            custom_layer = SoftPlus()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32) # Больший диапазон\n",
    "            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "            # 1. Проверка выхода\n",
    "            custom_layer_output = custom_layer.forward(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            # Softplus может давать большие значения, увеличим rtol\n",
    "            self.assertNumpyClose(custom_layer_output, torch_layer_output_var.data.numpy(), rtol=1e-4, msg=\"SoftPlus forward\")\n",
    "\n",
    "            # 2. Проверка градиента по входу\n",
    "            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            self.assertNumpyClose(custom_layer_grad, torch_layer_grad_var.data.numpy(), rtol=1e-4, msg=\"SoftPlus gradInput\")\n",
    "\n",
    "\n",
    "    def test_ClassNLLCriterionUnstable(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 5, 10 # Больше классов\n",
    "        for _ in range(100):\n",
    "            # Используем нашу реализацию NLL, сравниваем с PyTorch NLLLoss\n",
    "            custom_criterion = ClassNLLCriterionUnstable()\n",
    "            # PyTorch NLLLoss ожидает log-вероятности, поэтому берем log от нашего входа\n",
    "            torch_criterion = torch.nn.NLLLoss()\n",
    "\n",
    "            # Генерируем вероятности (выход SoftMax)\n",
    "            logits = np.random.uniform(-2, 2, (batch_size, n_in)).astype(np.float32)\n",
    "            probs = SoftMax().forward(logits) # Используем нашу реализацию SoftMax\n",
    "            # Убедимся, что вероятности корректны\n",
    "            probs = np.clip(probs, custom_criterion.EPS, 1. - custom_criterion.EPS)\n",
    "            probs /= probs.sum(axis=-1, keepdims=True) # Нормализуем на всякий случай\n",
    "\n",
    "            # Генерируем one-hot target\n",
    "            target_labels = np.random.randint(0, n_in, batch_size)\n",
    "            target_one_hot = np.zeros((batch_size, n_in), np.float32)\n",
    "            target_one_hot[np.arange(batch_size), target_labels] = 1.0\n",
    "\n",
    "            # 1. Проверка значения лосса\n",
    "            custom_loss = custom_criterion.forward(probs, target_one_hot)\n",
    "            # Для PyTorch нужен log(probs) и метки классов (не one-hot)\n",
    "            log_probs_torch = torch.log(torch.from_numpy(probs))\n",
    "            target_labels_torch = torch.from_numpy(target_labels).long()\n",
    "            torch_loss = torch_criterion(log_probs_torch, target_labels_torch)\n",
    "            self.assertAlmostEqual(custom_loss, torch_loss.item(), delta=1e-6, msg=\"NLLUnstable forward\")\n",
    "\n",
    "            # 2. Проверка градиента по входу (probs)\n",
    "            custom_grad_input = custom_criterion.backward(probs, target_one_hot)\n",
    "            # Чтобы получить градиент по probs в PyTorch, нужно сделать requires_grad=True для log_probs\n",
    "            log_probs_torch_var = Variable(log_probs_torch.data, requires_grad=True)\n",
    "            log_probs_torch_var.retain_grad() # Нужно для промежуточных переменных\n",
    "            # dL/d(probs) = dL/d(log_probs) * d(log_probs)/d(probs) = grad(log_probs) / probs\n",
    "            torch_loss_for_grad = torch_criterion(log_probs_torch_var, target_labels_torch)\n",
    "            torch_loss_for_grad.backward()\n",
    "            torch_grad_log_probs = log_probs_torch_var.grad.data.numpy()\n",
    "            # Численно нестабильно делить на probs, сравним с нашей формулой: -target / (probs * N)\n",
    "            expected_torch_grad = - target_one_hot / (probs * batch_size)\n",
    "            # Сравниваем наш градиент с теоретическим градиентом PyTorch\n",
    "            self.assertNumpyClose(custom_grad_input, expected_torch_grad, atol=1e-6, msg=\"NLLUnstable gradInput\")\n",
    "\n",
    "\n",
    "    def test_ClassNLLCriterion(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 5, 10\n",
    "        for _ in range(100):\n",
    "            # Наша реализация NLL (стабильная)\n",
    "            custom_criterion = ClassNLLCriterion()\n",
    "            # PyTorch NLLLoss\n",
    "            torch_criterion = torch.nn.NLLLoss()\n",
    "\n",
    "            # Генерируем log-вероятности (выход LogSoftMax)\n",
    "            logits = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "            log_probs = LogSoftMax().forward(logits) # Используем нашу реализацию LogSoftMax\n",
    "\n",
    "            # Генерируем one-hot target\n",
    "            target_labels = np.random.randint(0, n_in, batch_size)\n",
    "            target_one_hot = np.zeros((batch_size, n_in), np.float32)\n",
    "            target_one_hot[np.arange(batch_size), target_labels] = 1.0\n",
    "\n",
    "            # 1. Проверка значения лосса\n",
    "            custom_loss = custom_criterion.forward(log_probs, target_one_hot)\n",
    "            # PyTorch принимает log-вероятности и метки классов\n",
    "            log_probs_torch = torch.from_numpy(log_probs)\n",
    "            target_labels_torch = torch.from_numpy(target_labels).long()\n",
    "            torch_loss = torch_criterion(log_probs_torch, target_labels_torch)\n",
    "            self.assertAlmostEqual(custom_loss, torch_loss.item(), delta=1e-6, msg=\"NLLStable forward\")\n",
    "\n",
    "            # 2. Проверка градиента по входу (log_probs)\n",
    "            custom_grad_input = custom_criterion.backward(log_probs, target_one_hot)\n",
    "            # PyTorch NLLLoss сразу дает градиент по log_probs\n",
    "            log_probs_torch_var = Variable(log_probs_torch.data, requires_grad=True)\n",
    "            torch_loss_for_grad = torch_criterion(log_probs_torch_var, target_labels_torch)\n",
    "            torch_loss_for_grad.backward()\n",
    "            torch_grad_input = log_probs_torch_var.grad.data.numpy()\n",
    "            # Сравниваем градиенты напрямую\n",
    "            self.assertNumpyClose(custom_grad_input, torch_grad_input, atol=1e-7, msg=\"NLLStable gradInput\") # Повышенная точность\n",
    "\n",
    "\n",
    "    def test_MaxPool2d(self):\n",
    "        hyperparams = [\n",
    "            {'batch_size': 2, 'channels': 3, 'height': 5, 'width': 6, 'kernel_size': 2, 'stride': 2, 'padding': 0},\n",
    "            {'batch_size': 4, 'channels': 1, 'height': 7, 'width': 7, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "            {'batch_size': 2, 'channels': 2, 'height': 8, 'width': 6, 'kernel_size': (2,3), 'stride': (2,1), 'padding': (1,0)},\n",
    "            {'batch_size': 1, 'channels': 1, 'height': 4, 'width': 4, 'kernel_size': 4, 'stride': 4, 'padding': 0}, # Global-like\n",
    "        ]\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        for params in hyperparams:\n",
    "          with self.subTest(params=params):\n",
    "              kernel_size = params['kernel_size']\n",
    "              stride = params['stride']\n",
    "              padding = params['padding']\n",
    "\n",
    "              custom_module = MaxPool2d(kernel_size, stride, padding)\n",
    "              custom_module.train() # Не влияет, но для консистентности\n",
    "\n",
    "              # PyTorch MaxPool2d с return_indices=True для сравнения градиентов\n",
    "              torch_module = torch.nn.MaxPool2d(kernel_size, stride=stride, padding=padding, return_indices=False) # Пока без индексов\n",
    "              torch_module.train()\n",
    "\n",
    "              input_np = np.random.randn(params['batch_size'], params['channels'], params['height'], params['width']).astype(np.float32)\n",
    "              input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "              # 1. Проверка выхода\n",
    "              custom_output = custom_module.forward(input_np)\n",
    "              torch_output = torch_module(input_var)\n",
    "              self.assertNumpyClose(custom_output, torch_output.detach().numpy(), atol=1e-7, msg=f\"MaxPool2d forward {params}\") # Повышенная точность\n",
    "\n",
    "              # 2. Проверка градиента по входу\n",
    "              next_grad = np.random.randn(*custom_output.shape).astype(np.float32)\n",
    "              custom_grad = custom_module.backward(input_np, next_grad)\n",
    "              torch_output.backward(torch.tensor(next_grad))\n",
    "              torch_grad = input_var.grad.detach().numpy()\n",
    "              # Для MaxPool градиенты могут сильно различаться при одинаковых значениях в окне\n",
    "              # Снижаем требования к точности\n",
    "              self.assertNumpyClose(custom_grad, torch_grad, atol=1e-6, rtol=1e-5, msg=f\"MaxPool2d gradInput {params}\")\n",
    "\n",
    "\n",
    "    def test_AvgPool2d(self):\n",
    "        hyperparams = [\n",
    "            {'batch_size': 2, 'channels': 3, 'height': 5, 'width': 6, 'kernel_size': 2, 'stride': 2, 'padding': 0},\n",
    "            {'batch_size': 4, 'channels': 1, 'height': 7, 'width': 7, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "            {'batch_size': 2, 'channels': 2, 'height': 8, 'width': 6, 'kernel_size': (2,3), 'stride': (2,1), 'padding': (1,0)},\n",
    "             {'batch_size': 1, 'channels': 1, 'height': 4, 'width': 4, 'kernel_size': 4, 'stride': 4, 'padding': 0}, # Global-like\n",
    "        ]\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        for params in hyperparams:\n",
    "          with self.subTest(params=params):\n",
    "              kernel_size = params['kernel_size']\n",
    "              stride = params['stride']\n",
    "              padding = params['padding']\n",
    "\n",
    "              custom_module = AvgPool2d(kernel_size, stride, padding)\n",
    "              custom_module.train()\n",
    "\n",
    "              torch_module = torch.nn.AvgPool2d(kernel_size, stride=stride, padding=padding)\n",
    "              torch_module.train()\n",
    "\n",
    "              input_np = np.random.randn(params['batch_size'], params['channels'], params['height'], params['width']).astype(np.float32)\n",
    "              input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "              # 1. Проверка выхода\n",
    "              custom_output = custom_module.forward(input_np)\n",
    "              torch_output = torch_module(input_var)\n",
    "              self.assertNumpyClose(custom_output, torch_output.detach().numpy(), atol=1e-7, msg=f\"AvgPool2d forward {params}\")\n",
    "\n",
    "              # 2. Проверка градиента по входу\n",
    "              next_grad = np.random.randn(*custom_output.shape).astype(np.float32)\n",
    "              custom_grad = custom_module.backward(input_np, next_grad)\n",
    "              torch_output.backward(torch.tensor(next_grad))\n",
    "              torch_grad = input_var.grad.detach().numpy()\n",
    "              self.assertNumpyClose(custom_grad, torch_grad, atol=1e-6, rtol=1e-5, msg=f\"AvgPool2d gradInput {params}\")\n",
    "\n",
    "\n",
    "    def test_GlobalMaxPool2d(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "        batch_size, channels, height, width = 4, 3, 8, 6 # Разные H, W\n",
    "\n",
    "        for _ in range(10): # Меньше итераций, т.к. нет параметров\n",
    "          custom_module = GlobalMaxPool2d()\n",
    "          # Эквивалент в PyTorch - AdaptiveMaxPool2d с output_size=(1, 1)\n",
    "          torch_module = torch.nn.AdaptiveMaxPool2d((1, 1))\n",
    "\n",
    "          input_np = np.random.randn(batch_size, channels, height, width).astype(np.float32)\n",
    "          input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "          # 1. Проверка выхода\n",
    "          # Наш модуль возвращает (N, C), PyTorch возвращает (N, C, 1, 1)\n",
    "          custom_output = custom_module.forward(input_np)\n",
    "          torch_output = torch_module(input_var)\n",
    "          torch_output_squeezed = torch_output.squeeze(dim=(2, 3)) # Убираем оси (1, 1)\n",
    "          self.assertNumpyClose(custom_output, torch_output_squeezed.detach().numpy(), atol=1e-7, msg=\"GlobalMaxPool2d forward\")\n",
    "\n",
    "          # 2. Проверка градиента по входу\n",
    "          # gradOutput для нашего модуля - (N, C)\n",
    "          next_grad_custom = np.random.randn(*custom_output.shape).astype(np.float32)\n",
    "          # gradOutput для PyTorch - (N, C, 1, 1)\n",
    "          next_grad_torch = torch.tensor(next_grad_custom.reshape(batch_size, channels, 1, 1))\n",
    "\n",
    "          custom_grad = custom_module.backward(input_np, next_grad_custom)\n",
    "          torch_output.backward(next_grad_torch)\n",
    "          torch_grad = input_var.grad.detach().numpy()\n",
    "          # Снижаем требования к точности для MaxPool\n",
    "          self.assertNumpyClose(custom_grad, torch_grad, atol=1e-6, rtol=1e-5, msg=\"GlobalMaxPool2d gradInput\")\n",
    "\n",
    "\n",
    "    def test_GlobalAvgPool2d(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "        batch_size, channels, height, width = 4, 3, 8, 6 # Разные H, W\n",
    "\n",
    "        for _ in range(10):\n",
    "          custom_module = GlobalAvgPool2d()\n",
    "          torch_module = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "          input_np = np.random.randn(batch_size, channels, height, width).astype(np.float32)\n",
    "          input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "          # 1. Проверка выхода\n",
    "          custom_output = custom_module.forward(input_np) # (N, C)\n",
    "          torch_output = torch_module(input_var) # (N, C, 1, 1)\n",
    "          torch_output_squeezed = torch_output.squeeze(dim=(2, 3))\n",
    "          self.assertNumpyClose(custom_output, torch_output_squeezed.detach().numpy(), atol=1e-7, msg=\"GlobalAvgPool2d forward\")\n",
    "\n",
    "          # 2. Проверка градиента по входу\n",
    "          next_grad_custom = np.random.randn(*custom_output.shape).astype(np.float32) # (N, C)\n",
    "          next_grad_torch = torch.tensor(next_grad_custom.reshape(batch_size, channels, 1, 1)) # (N, C, 1, 1)\n",
    "\n",
    "          custom_grad = custom_module.backward(input_np, next_grad_custom)\n",
    "          torch_output.backward(next_grad_torch)\n",
    "          torch_grad = input_var.grad.detach().numpy()\n",
    "          self.assertNumpyClose(custom_grad, torch_grad, atol=1e-6, rtol=1e-5, msg=\"GlobalAvgPool2d gradInput\")\n",
    "\n",
    "\n",
    "    def test_Flatten(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Тестируем только стандартный Flatten (с start_dim=1)\n",
    "        custom_module = Flatten()\n",
    "        # PyTorch Flatten по умолчанию тоже использует start_dim=1\n",
    "        torch_module = torch.nn.Flatten()\n",
    "\n",
    "        # Разные формы входа\n",
    "        input_shapes = [(2, 3, 4, 5), (10, 1), (5, 2, 1, 3)]\n",
    "\n",
    "        for shape in input_shapes:\n",
    "          with self.subTest(shape=shape):\n",
    "              input_np = np.random.randn(*shape).astype(np.float32)\n",
    "              input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "              # 1. Проверка выхода\n",
    "              custom_output = custom_module.forward(input_np)\n",
    "              torch_output = torch_module(input_var)\n",
    "              self.assertNumpyClose(custom_output, torch_output.detach().numpy(), msg=f\"Flatten forward {shape}\")\n",
    "\n",
    "              # 2. Проверка градиента по входу\n",
    "              next_grad = np.random.randn(*custom_output.shape).astype(np.float32)\n",
    "              custom_grad = custom_module.backward(input_np, next_grad)\n",
    "              torch_output.backward(torch.tensor(next_grad))\n",
    "              torch_grad = input_var.grad.detach().numpy()\n",
    "              self.assertNumpyClose(custom_grad, torch_grad, msg=f\"Flatten gradInput {shape}\")\n",
    "\n",
    "\n",
    "    def test_Gelu(self):\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in = 10, 5\n",
    "        for _ in range(100):\n",
    "          custom_module = Gelu()\n",
    "          custom_module.train() # Не влияет, но для консистентности\n",
    "\n",
    "          torch_module = torch.nn.GELU(approximate='none')\n",
    "          torch_module.train()\n",
    "\n",
    "          input_np = np.random.randn(batch_size, n_in).astype(np.float32)\n",
    "          input_var = torch.tensor(input_np, requires_grad=True)\n",
    "\n",
    "          # 1. Проверка выхода\n",
    "          custom_output = custom_module.forward(input_np)\n",
    "          torch_output = torch_module(input_var)\n",
    "          # Увеличиваем atol до 1e-6 из-за возможных различий в точности erf\n",
    "          self.assertNumpyClose(custom_output, torch_output.detach().numpy(), atol=1e-6, msg=\"Gelu forward\")\n",
    "\n",
    "          # 2. Проверка градиента по входу\n",
    "          next_grad = np.random.randn(*custom_output.shape).astype(np.float32)\n",
    "          custom_grad = custom_module.backward(input_np, next_grad)\n",
    "          torch_output.backward(torch.tensor(next_grad))\n",
    "          torch_grad = input_var.grad.detach().numpy()\n",
    "          self.assertNumpyClose(custom_grad, torch_grad, atol=1e-6, rtol=1e-5, msg=\"Gelu gradInput\")\n",
    "\n",
    "\n",
    "# Запуск тестов\n",
    "if __name__ == '__main__':\n",
    "    # unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "    # Запускаем тесты и выводим результат\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestLayers)\n",
    "    runner = unittest.TextTestRunner(verbosity=2) # verbosity=2 для более детального вывода\n",
    "    result = runner.run(suite)\n",
    "\n",
    "    # Печатаем итоги\n",
    "    print(\"\\n--- Test Summary ---\")\n",
    "    print(f\"Ran: {result.testsRun} tests\")\n",
    "    if result.wasSuccessful():\n",
    "        print(\"Result: OK\")\n",
    "    else:\n",
    "        print(\"Result: FAILED\")\n",
    "        print(f\"Errors: {len(result.errors)}\")\n",
    "        for test, err in result.errors:\n",
    "            print(f\"\\nERROR in {test}:\\n{err}\")\n",
    "        print(f\"Failures: {len(result.failures)}\")\n",
    "        for test, fail in result.failures:\n",
    "            print(f\"\\nFAILURE in {test}:\\n{fail}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
